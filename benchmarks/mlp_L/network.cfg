[net]
batch=256
height=128
width=128
channels=3
momentum=0.9
decay=0.0001
learning_rate=0.01
num_iterations=731168

[connected]
output=2500
activation = relu

[connected]
output=2000
activation = relu

[connected]
output=1500
activation = relu

[connected]
output=1000
activation = relu

[connected]
output=500
activation = relu

[connected]
output=62
activation = linear

[softmax]
groups=1

[cost]
type=l2
