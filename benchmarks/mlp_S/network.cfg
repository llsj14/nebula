[net]
batch=32
subdivisions=1
height=32
width=32
channels=3
momentum=0.9
decay=0.0001
learning_rate=0.01
#num_iterations=5526
num_iterations=5

[connected]
output=1250
activation = relu

[connected]
output=1000
activation = relu

[connected]
output=750
activation = relu

[connected]
output=10
activation = linear

[softmax]
groups=1

[cost]
type=l2
